{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Founded in 2015, Nota AI is a compressed AI solutions and software optimization platform business focusing on the B2B and B2G markets. Our mission is to accelerate and maximize the everyday use of AI in the global market.</p> <p>Our platform solution, NetsPresso, resolves server-based AI issues, such as limited networks, huge costs, and privacy breaches. NetsPresso automatically downsizes computer vision models to a size small enough to be deployed independently on low-specification edge devices.</p> <p>With the power of edge AI, Nota also empowers intelligent transportation systems (ITS) with driver monitoring systems and security &amp; surveillance with face recognition.</p> <p>Time for every day and everywhere AI with Nota!</p>"},{"location":"docker-installation/","title":"Docker","text":""},{"location":"docker-installation/#installation-with-docker","title":"Installation with docker","text":""},{"location":"docker-installation/#docker-with-docker-compose","title":"Docker with docker-compose","text":"<p>For the latest information, please check <code>docker-compose.yml</code></p> <pre><code># run command\nexport TAG=v$(cat src/netspresso_trainer/VERSION) &amp;&amp; \\\ndocker compose run --service-ports --name netspresso-trainer-dev netspresso-trainer bash\n</code></pre>"},{"location":"docker-installation/#docker-image-build","title":"Docker image build","text":"<p>If you run with <code>docker run</code> command, follow the image build and run command in the below:</p> <pre><code># build an image\ndocker build -t netspresso-trainer:v$(cat src/netspresso_trainer/VERSION) .\n</code></pre> <pre><code># docker run command\ndocker run -it --ipc=host\\\n--gpus='\"device=0,1,2,3\"'\\\n-v /PATH/TO/DATA:/DATA/PATH/IN/CONTAINER\\\n-v /PATH/TO/CHECKPOINT:/CHECKPOINT/PATH/IN/CONTAINER\\\n-p 50001:50001\\\n-p 50002:50002\\\n-p 50003:50003\\\n--name netspresso-trainer-dev netspresso-trainer:v$(cat src/netspresso_trainer/VERSION)\n</code></pre>"},{"location":"getting-started/","title":"Simple use","text":""},{"location":"getting-started/#getting-started","title":"Getting started","text":"<p>Write your training script in <code>train.py</code> like:</p> <pre><code>from netspresso_trainer import set_arguments, train\nargs_parsed, args = set_arguments(is_graphmodule_training=False)\ntrain(args_parsed, args, is_graphmodule_training=False)\n</code></pre> <p>Then, train your model with your own configuraiton:</p> <pre><code>python train.py\\\n--data config/data/beans.yaml\\\n--augmentation config/augmentation/resnet.yaml\\\n--model config/model/resnet.yaml\\\n--training config/training/resnet.yaml\\\n--logging config/logging.yaml\\\n--environment config/environment.yaml\n</code></pre> <p>Please refer to <code>example_train.sh</code> and <code>example_train_fx.sh</code>.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python <code>3.8</code> | <code>3.9</code> | <code>3.10</code></li> <li>PyTorch <code>1.13.0</code> (recommended) (compatible with: <code>1.11.x</code> - <code>1.13.x</code>)</li> </ul>"},{"location":"installation/#install-with-pypi-stable","title":"Install with pypi (stable)","text":"<pre><code>pip install netspresso_trainer\n</code></pre>"},{"location":"installation/#install-with-github","title":"Install with GitHub","text":"<pre><code>pip install git+https://github.com:Nota-NetsPresso/netspresso-trainer.git@stable\n</code></pre> <p>To install with editable mode,</p> <pre><code>git clone https://github.com:Nota-NetsPresso/netspresso-trainer.git .\npip install -e netspresso-trainer\n</code></pre>"},{"location":"installation/#set-up-with-docker","title":"Set-up with docker","text":"<p>Please clone this repository and refer to <code>Dockerfile</code> and <code>docker-compose-example.yml</code>. For docker users, we provide more detailed guide with <code>DOCKER-INSTALLATION.md</code>.</p>"},{"location":"training/","title":"Index","text":""},{"location":"training/#example-training","title":"Example training","text":"<p>This code provides some example scripts and snippets to help you understand about the functionalities.</p>"},{"location":"training/#training-example-model","title":"Training example model","text":"<p>For classification and segmentation, see <code>train_classification.sh</code> and <code>train_segmentation.sh</code> for each. Each shell sciprt contains two commands: (1) single-gpu training and (2) multi-gpu training. A default option is using single-gpu, but you can edit the script if you needed.</p> <p> <code>2023.06.21</code> Work in progress for detection (It won't work for now)</p>"},{"location":"training/huggingface-datasets/","title":"Hugging Face datasets","text":""},{"location":"training/huggingface-datasets/#training-with-huggingface-datasets","title":"Training with HuggingFace datasets","text":"<p>We do our best to give you a good experience in training process. We integrate HuggingFace(HF) datasets into our training pipeline. Note that we apply our custom augmentation methods in training datasets, instead of albumentations which is mostly used in HF datasets.</p> <p>To do so, firstly you need to install additional libraries with the following command:</p> <pre><code>pip install -r requirements-data.txt\n</code></pre> <p>Then, you can write your own data configuration for HF datasets. Please refer to data configuration template. Some datasets in HF datasets needs <code>login</code>. You can login with <code>huggingface-cli login</code> with their official guide.</p>"},{"location":"training/tensorboard/","title":"Tensorboard","text":""},{"location":"training/tensorboard/#tensorboard","title":"Tensorboard","text":"<p>We provide basic tensorboard to track your training status. Run the tensorboard with the following command: </p> <pre><code>tensorboard --logdir ./outputs --port 50001 --bind_all\n</code></pre> <p>where <code>PORT</code> for tensorboard is 50001. Note that the default directory of saving result will be <code>./outputs</code> directory.</p>"}]}