{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Founded in 2015, Nota AI is a compressed AI solutions and software optimization platform business focusing on the B2B and B2G markets. Our mission is to accelerate and maximize the everyday use of AI in the global market.</p> <p>Our platform solution, NetsPresso, resolves server-based AI issues, such as limited networks, huge costs, and privacy breaches. NetsPresso automatically downsizes computer vision models to a size small enough to be deployed independently on low-specification edge devices.</p> <p>With the power of edge AI, Nota also empowers intelligent transportation systems (ITS) with driver monitoring systems and security &amp; surveillance with face recognition.</p> <p>Time for every day and everywhere AI with Nota!</p>"},{"location":"docker-installation/","title":"Docker","text":""},{"location":"docker-installation/#installation-with-docker","title":"Installation with docker","text":""},{"location":"docker-installation/#docker-with-docker-compose","title":"Docker with docker-compose","text":"<p>For the latest information, please check <code>docker-compose.yml</code></p> <pre><code># run command\nexport TAG=v$(cat src/netspresso_trainer/VERSION) &amp;&amp; \\\ndocker compose run --service-ports --name netspresso-trainer-dev netspresso-trainer bash\n</code></pre>"},{"location":"docker-installation/#docker-image-build","title":"Docker image build","text":"<p>If you run with <code>docker run</code> command, follow the image build and run command in the below:</p> <pre><code># build an image\ndocker build -t netspresso-trainer:v$(cat src/netspresso_trainer/VERSION) .\n</code></pre> <pre><code># docker run command\ndocker run -it --ipc=host\\\n--gpus='\"device=0,1,2,3\"'\\\n-v /PATH/TO/DATA:/DATA/PATH/IN/CONTAINER\\\n-v /PATH/TO/CHECKPOINT:/CHECKPOINT/PATH/IN/CONTAINER\\\n-p 50001:50001\\\n-p 50002:50002\\\n-p 50003:50003\\\n--name netspresso-trainer-dev netspresso-trainer:v$(cat src/netspresso_trainer/VERSION)\n</code></pre>"},{"location":"getting-started/","title":"Simple use","text":""},{"location":"getting-started/#getting-started","title":"Getting started","text":"<p>Write your training script in <code>train.py</code> like:</p> <pre><code>from netspresso_trainer import set_arguments, train\nargs_parsed, args = set_arguments(is_graphmodule_training=False)\ntrain(args_parsed, args, is_graphmodule_training=False)\n</code></pre> <p>Then, train your model with your own configuraiton:</p> <pre><code>python train.py\\\n--data config/data/beans.yaml\\\n--augmentation config/augmentation/resnet.yaml\\\n--model config/model/resnet.yaml\\\n--training config/training/resnet.yaml\\\n--logging config/logging.yaml\\\n--environment config/environment.yaml\n</code></pre> <p>Please refer to <code>example_train.sh</code> and <code>example_train_fx.sh</code>.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python <code>3.8</code> | <code>3.9</code> | <code>3.10</code></li> <li>PyTorch <code>1.13.0</code> (recommended) (compatible with: <code>1.11.x</code> - <code>1.13.x</code>)</li> </ul>"},{"location":"installation/#install-with-pypi-stable","title":"Install with pypi (stable)","text":"<pre><code>pip install netspresso_trainer\n</code></pre>"},{"location":"installation/#install-with-github","title":"Install with GitHub","text":"<pre><code>pip install git+https://github.com:Nota-NetsPresso/netspresso-trainer.git@stable\n</code></pre> <p>To install with editable mode,</p> <pre><code>git clone https://github.com:Nota-NetsPresso/netspresso-trainer.git .\npip install -e netspresso-trainer\n</code></pre>"},{"location":"installation/#set-up-with-docker","title":"Set-up with docker","text":"<p>Please clone this repository and refer to <code>Dockerfile</code> and <code>docker-compose-example.yml</code>. For docker users, we provide more detailed guide with <code>DOCKER-INSTALLATION.md</code>.</p>"},{"location":"models/","title":"Index","text":""},{"location":"models/#models","title":"Models","text":""},{"location":"models/#features","title":"Features","text":"<ul> <li>Compatible with torch.fx converting</li> <li>Can be compressed with pruning method provided in NetsPresso</li> <li>Efficient to be easily deployed at many edge devices.</li> </ul>"},{"location":"models/#vit-and-metaformer","title":"ViT and MetaFormer","text":"<p>MetaFormer, introduced by Sea AI Lab, suggested the abstracted architecture of Vision Transformer and has widely applied in recent vision backbone architectures with better performance. The framework of MetaFormer not just covers the original ViT based models, but also explains some hybrid models of ConvNets and ViT in single framework. Especially, those models set new record in Efficient vision task, which show better performance in both accuracy and inference speed.  </p> <p>From the MetaFormer paper, the concept of MetaFormer can be expressed as follows:</p> \\[ X = \\mathrm{InputEmbedding}(I), \\newline X' = \\mathrm{MetaFormerEncoder}(X), \\newline X'' = \\mathrm{NormOrIdentity_{output}}(X'), \\] <p>where \\(\\mathrm{MetaFormerEncoder}\\) usually consists of repeated MetaFormer blocks.</p> <p>One of MetaFormer blocks can be expressed as follows:</p> <p>$$ X' = X + \\mathrm{TokenMixer}\\left(\\mathrm{Norm_1}(X)\\right), \\newline X'' = X' + \\mathrm{ChannelMLP}\\left(\\mathrm{Norm_2}(X')\\right), $$ where \\(\\mathrm{TokenMixer}\\) could be either \\(\\mathrm{MultiHeadSelfAttention}\\), \\(\\mathrm{Identity}\\), or \\(\\mathrm{ConvLayers}\\), and \\(\\mathrm{ChannelMLP}\\) usually consists of <code>Linear</code> layers and activation function.</p> <p>Inspired from the MetaFormer research, we try out best to design and build models (both ViT and non-ViT models) with MetaFormer framework to utilize in various ways. By defining models with unified MetaFormer block, we can apply to all models whenever NetsPresso services progressed. For example, Compressor API of PyNetsPresso supports structural pruning for those models, which accelerates the inference only with negotiable performance drop.</p>"},{"location":"models/#retraining-the-model-from-netspresso","title":"Retraining the model from NetsPresso","text":"<p>If you got compressed model from NetsPresso, then it's time to retrain your model to get the best performance.</p>"},{"location":"models/implementations/","title":"Implementations","text":""},{"location":"models/model-list/","title":"Index","text":""},{"location":"models/model-list/#pretrained-weights","title":"Pretrained weights","text":"<p>For now, we provide the pretrained weight from other awesome repositories. We have converted several models' weights into our own model architectures. In the near soon, we are planning to provide the pretrained weights directly trained from our resources. We appreciate all the original authors and we also do our best to make other values.</p> <p>Download all weights (Google Drvie)</p> Family Model Link Origianl repository ResNet <code>resnet50</code> Google Drive torchvision ViT <code>vit_tiny</code> Google Drive apple/ml-cvnets MobileViT <code>mobilevit_s</code> Google Drive apple/ml-cvnets SegFormer <code>segformer</code> Google Drive (Hugging Face) nvidia EfficientForemer <code>efficientformer_l1_3000d</code> Google Drive snap-research/EfficientFormer PIDNet <code>pidnet_s</code> Google Drive XuJiacong/PIDNet"},{"location":"models/model-list/backbones/","title":"Backbones","text":""},{"location":"models/model-list/backbones/#models","title":"Models","text":""},{"location":"models/model-list/backbones/#resnet","title":"ResNet","text":"Model torch.fx Supporting Task(s) Checkpoint <code>resnet50</code> \u2705 Classification Google Drive"},{"location":"models/model-list/backbones/#vit","title":"ViT","text":"Model torch.fx Supporting Task(s) Checkpoint <code>vit_tiny</code> \u2705 Classification Google Drive"},{"location":"models/model-list/backbones/#mobilevit","title":"MobileViT","text":"Model torch.fx Supporting Task(s) Checkpoint <code>mobilevit_s</code> \u2705 Classification Google Drive"},{"location":"models/model-list/backbones/#segformer","title":"SegFormer","text":"Model torch.fx Supporting Task(s) Checkpoint <code>segformer</code> \u2705 Classification,Segmentation Google Drive"},{"location":"models/model-list/backbones/#efficientformer","title":"EfficientFormer","text":"Model torch.fx Supporting Task(s) Checkpoint <code>efficientformer_l1</code> \u2705 Classification,Segmentation Google Drive"},{"location":"models/model-list/full-models/","title":"Full models","text":""},{"location":"models/model-list/full-models/#models","title":"Models","text":""},{"location":"models/model-list/full-models/#pidnet","title":"PIDNet","text":"Model torch.fx Supporting Task(s) Checkpoint <code>pidnet_s</code> \u2705 Segmentation Google Drive"},{"location":"models/model-list/heads/","title":"Heads","text":""},{"location":"models/model-list/heads/#head-architecutres","title":"Head Architecutres","text":""},{"location":"models/model-list/heads/#classification","title":"Classification","text":"Architecture torch.fx Supporting Models <code>fc</code> \u2705 (All backbones)"},{"location":"models/model-list/heads/#segmentation","title":"Segmentation","text":"Architecture torch.fx Supporting Models <code>segment_head</code> \u2705 SegFormerEfficientFormer"},{"location":"models/model-list/heads/#detection","title":"Detection","text":"<p> <code>2023.08.08</code> Work in progress for detection</p>"},{"location":"training/","title":"Index","text":""},{"location":"training/#example-training","title":"Example training","text":"<p>This code provides some example scripts and snippets to help you understand about the functionalities.</p>"},{"location":"training/#training-example-model","title":"Training example model","text":"<p>For classification and segmentation, see <code>train_classification.sh</code> and <code>train_segmentation.sh</code> for each. Each shell sciprt contains two commands: (1) single-gpu training and (2) multi-gpu training. A default option is using single-gpu, but you can edit the script if you needed.</p> <p> <code>2023.06.21</code> Work in progress for detection (It won't work for now)</p>"},{"location":"training/huggingface-datasets/","title":"Hugging Face datasets","text":""},{"location":"training/huggingface-datasets/#training-with-huggingface-datasets","title":"Training with HuggingFace datasets","text":"<p>We do our best to give you a good experience in training process. We integrate HuggingFace(HF) datasets into our training pipeline. Note that we apply our custom augmentation methods in training datasets, instead of albumentations which is mostly used in HF datasets.</p> <p>To do so, firstly you need to install additional libraries with the following command:</p> <pre><code>pip install -r requirements-data.txt\n</code></pre> <p>Then, you can write your own data configuration for HF datasets. Please refer to data configuration template. Some datasets in HF datasets needs <code>login</code>. You can login with <code>huggingface-cli login</code> with their official guide.</p>"},{"location":"training/tensorboard/","title":"Tensorboard","text":""},{"location":"training/tensorboard/#tensorboard","title":"Tensorboard","text":"<p>We provide basic tensorboard to track your training status. Run the tensorboard with the following command: </p> <pre><code>tensorboard --logdir ./outputs --port 50001 --bind_all\n</code></pre> <p>where <code>PORT</code> for tensorboard is 50001. Note that the default directory of saving result will be <code>./outputs</code> directory.</p>"}]}