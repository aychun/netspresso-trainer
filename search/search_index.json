{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>NetsPresso Trainer is a PyTorch training repository specialized for model training with torch.fx graphmodule conversion and automatic compression.  </p> <p>By using NetsPresso Trainer and NetsPresso together, you can take advantage of features such as model compression without significant performance drop, ONNX export, and NVIDIA TensorRT export.</p> <p>Enjoy training and deploying your own edge AI models with NetsPresso Trainer!</p>"},{"location":"docker-installation/","title":"Docker","text":""},{"location":"docker-installation/#installation-with-docker","title":"Installation with docker","text":""},{"location":"docker-installation/#docker-with-docker-compose","title":"Docker with docker-compose","text":"<p>For the latest information, please check <code>docker-compose.yml</code></p> <pre><code># run command\nexport TAG=v$(cat src/netspresso_trainer/VERSION) &amp;&amp; \\\ndocker compose run --service-ports --name netspresso-trainer-dev netspresso-trainer bash\n</code></pre>"},{"location":"docker-installation/#docker-image-build","title":"Docker image build","text":"<p>If you run with <code>docker run</code> command, follow the image build and run command in the below:</p> <pre><code># build an image\ndocker build -t netspresso-trainer:v$(cat src/netspresso_trainer/VERSION) .\n</code></pre> <pre><code># docker run command\ndocker run -it --ipc=host\\\n--gpus='\"device=0,1,2,3\"'\\\n-v /PATH/TO/DATA:/DATA/PATH/IN/CONTAINER\\\n-v /PATH/TO/CHECKPOINT:/CHECKPOINT/PATH/IN/CONTAINER\\\n-p 50001:50001\\\n-p 50002:50002\\\n-p 50003:50003\\\n--name netspresso-trainer-dev netspresso-trainer:v$(cat src/netspresso_trainer/VERSION)\n</code></pre>"},{"location":"getting-started/","title":"Simple use","text":""},{"location":"getting-started/#getting-started","title":"Getting started","text":"<p>Write your training script in <code>train.py</code> like:</p> <pre><code>from netspresso_trainer import set_arguments, train\nargs_parsed, args = set_arguments(is_graphmodule_training=False)\ntrain(args_parsed, args, is_graphmodule_training=False)\n</code></pre> <p>Then, train your model with your own configuraiton:</p> <pre><code>python train.py\\\n--data config/data/beans.yaml\\\n--augmentation config/augmentation/resnet.yaml\\\n--model config/model/resnet.yaml\\\n--training config/training/resnet.yaml\\\n--logging config/logging.yaml\\\n--environment config/environment.yaml\n</code></pre> <p>Please refer to <code>example_train.sh</code> and <code>example_train_fx.sh</code>.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python <code>3.8</code> | <code>3.9</code> | <code>3.10</code></li> <li>PyTorch <code>1.13.0</code> (recommended) (compatible with: <code>1.11.x</code> - <code>1.13.x</code>)</li> </ul>"},{"location":"installation/#install-with-pypi-stable","title":"Install with pypi (stable)","text":"<pre><code>pip install netspresso_trainer\n</code></pre>"},{"location":"installation/#install-with-github","title":"Install with GitHub","text":"<pre><code>pip install git+https://github.com:Nota-NetsPresso/netspresso-trainer.git@stable\n</code></pre> <p>To install with editable mode,</p> <pre><code>git clone https://github.com:Nota-NetsPresso/netspresso-trainer.git .\npip install -e netspresso-trainer\n</code></pre>"},{"location":"installation/#set-up-with-docker","title":"Set-up with docker","text":"<p>Please clone this repository and refer to <code>Dockerfile</code> and <code>docker-compose-example.yml</code>. For docker users, we provide more detailed guide with <code>DOCKER-INSTALLATION.md</code>.</p>"},{"location":"components/","title":"Index","text":"<p>FIXME</p> <p>Currently, NetsPresso Trainer supports training for three computer vision tasks; image classification, semantic segmentation, and object detection. </p>"},{"location":"components/augmentation/","title":"Overview","text":"<p>NetsPresso Trainer provides data augmentation function to improve model performance, allowing users to configure their own training recipes as desired.  Data augmentation in NetsPresso Trainer is based on torch and torchvision, and all augmentations are written based on <code>pillow</code> images and <code>torch.Tensor</code>.  </p> <p>Currently, there is no function for users to directly implement augmentation recipes, but only customize variable values to adjust the intensity and frequency of augmentations. In the near future, a function to directly design augmentation recipes will be added. </p>"},{"location":"components/augmentation/#supporting-transforms","title":"Supporting transforms","text":"<ul> <li>The currently supported augmentation methods in NetsPresso Trainer are as follows.</li> </ul>"},{"location":"components/augmentation/#colorjitter","title":"ColorJitter","text":""},{"location":"components/augmentation/#identity","title":"Identity","text":""},{"location":"components/augmentation/#normalize","title":"Normalize","text":""},{"location":"components/augmentation/#pad","title":"Pad","text":""},{"location":"components/augmentation/#padifneeded","title":"PadIfNeeded","text":""},{"location":"components/augmentation/#randomcrop","title":"RandomCrop","text":""},{"location":"components/augmentation/#randomhorizontalflip","title":"RandomHorizontalFlip","text":""},{"location":"components/augmentation/#randomresizedcrop","title":"RandomResizedCrop","text":""},{"location":"components/augmentation/#randomverticalflip","title":"RandomVerticalFlip","text":""},{"location":"components/augmentation/#resize","title":"Resize","text":""},{"location":"components/augmentation/#totensor","title":"ToTensor","text":""},{"location":"components/augmentation/#gradio-demo-for-simulating-the-transform","title":"Gradio demo for simulating the transform","text":"<p>In many learning function repositories, it is recommended to check the code and documentation for augmentations or actually run the training to check the logs to see how augmentations are performed.  NetsPresso Trainer supports augmentation simulation to help users easily understand the augmentation recipe they have configured.  By copying and entering the augmentation configuration into the simulator, users can preview how a specific image will be augmented in advance. However, transforms (e.g. Normalize, ToTensor) used to convert the image array for learning purposes are excluded from the simulation visualization process.  In particular, since this simulator directly imports the augmentation modules used in NetsPresso Trainer, users can use the same functions as the augmentation functions used in actual training to verify the results.  </p> <p>Our team hopes that the learning process with NetsPresso Trainer will become a more enjoyable experience for all users. </p>"},{"location":"components/augmentation/#how-to-use","title":"How to use","text":""},{"location":"components/augmentation/#running-on-your-environment","title":"Running on your environment","text":"<p>FIXME</p>"},{"location":"components/augmentation/#hugging-face-spaces","title":"Hugging Face Spaces","text":"<p>(HF Spaces demo link)</p>"},{"location":"components/augmentation/#field-list","title":"Field list","text":""},{"location":"components/augmentation/#common","title":"Common","text":"Field  Description <code>augmentation.img_size</code> (int) the image size of model input after finishing the data augmentation"},{"location":"components/augmentation/#resize_1","title":"Resize","text":"Field  Description <code>augmentation.crop_size_h</code> (int) the height of cropped image <code>augmentation.crop_size_w</code> (int) the width of cropped image"},{"location":"components/augmentation/#randomresizedcrop_1","title":"RandomResizedCrop","text":"Field  Description <code>augmentation.crop_size_h</code> (int) the height of cropped image <code>augmentation.crop_size_w</code> (int) the width of cropped image <code>augmentation.resize_ratio0</code> (float) the minimum scale of random image resizing <code>augmentation.resize_ratiof</code> (float) the maximum scale of random image resizing"},{"location":"components/augmentation/#randomhorizontalflip_1","title":"RandomHorizontalFlip","text":"Field  Description <code>augmentation.fliplr</code> (float) the probability of the flip. If <code>1.0</code>, it always flips the image."},{"location":"components/augmentation/#colorjitter_1","title":"ColorJitter","text":"Field  Description <code>augmentation.color_jitter.brightness</code> (float) the maximum scale of adjusting the brightness of an image. The scale value is selected within range. <code>augmentation.color_jitter.contrast</code> (float) the maximum scale of adjusting the contrast of an image. The scale value is selected within range. <code>augmentation.color_jitter.saturation</code> (float) the maximum scale of adjusting the saturation of an image. The scale value is selected within range. <code>augmentation.color_jitter.hue</code> (float) the maximum scale of adjusting the hue of an image. The scale value is selected within range. <code>augmentation.color_jitter.colorjitter_p</code> (float) the probability of applying color jitter. If <code>1.0</code>, it always applies the color transform."},{"location":"components/data/","title":"Data","text":""},{"location":"components/data/#overview","title":"Overview","text":"<p>NetsPresso Trainer supports learning functions for various vision tasks with your custom data.  In addition to data stored in a local repository, it also supports learning with data accessible through APIs such as Hugging Face datasets.  Currently, the dataset formats supported by NetsPresso Trainer are fixed in a specific form, but we plan to expand to more dataset formats such as COCO format in the future.  </p> <p>On this page, we will guide you on the data format you need to learn with your custom data and how to learn using Hugging Face datasets. </p>"},{"location":"components/data/#supporting-format","title":"Supporting format","text":"<p>For image data, various extension images are supported, but we recommend one of <code>.jpg</code>, <code>.jpeg</code>, <code>.png</code>, and <code>.bmp</code>. In this case, label data used in semantic segmentation must be saved as <code>.png</code> to prevent data loss and utilize image header information.  The following sections introduce how to organize data for each task. </p>"},{"location":"components/data/#image-classification","title":"Image classification","text":"<p>To train an image classification model using NetsPresso Trainer, the data must be in following formats: </p> <ul> <li>There must be a directory for each class to be distinguished by the classification model.</li> <li>Each class directory must contain all the images corresponding to that class.</li> <li>Collect directories containing images for each class under the root directory.</li> <li>Users must know in advance which class name each class directory name corresponds to.</li> </ul> <p>The example data directory structure for this is as follows:</p> <pre><code># TODO\n</code></pre> <p>An example yaml configuration for this is as follows:</p> <pre><code>data:\nname: food_pic\ntask: classification\nformat: local # local, huggingface\npath:\nroot: ./data/my_food_pics # dataset root\ntrain:\nimage: train # directory for training images\nlabel: ~  # label for training images\nvalid:\nimage: val  # directory for valid images\nlabel: ~  # label for valid images\ntest:\nimage: ~  # directory for test images\nlabel: ~  # label for test images\nid_mapping:  # Dict[directory_name, class_name]. If None, set the directory name same with class name\ndirectory_1: curry\ndirectory_2: ramen\ndirectory_3: rice\ndirectory_4: sushi\n</code></pre>"},{"location":"components/data/#semantic-segmentation","title":"Semantic segmentation","text":"<p>To train a semantic segmentation model using NetsPresso Trainer, the data must be in following formats: </p> <ul> <li>For each training image, there must be a label file (image) indicating the original image and the class index of each pixel of the image.</li> <li>Users must create an image and label directory under the root directory and put the corresponding files in each directory.</li> <li>In this case, training data and validation data can be distinguished in different directories. For example, training data can be placed in train/image, train/label directories, and validation data can be placed in valid/image, valid/label directories.</li> <li>Users must know the class name corresponding to each pixel value (RGB or L (grayscale) format) in the label file.</li> </ul> <p>The example data directory structure for this is as follows:</p> <pre><code># TODO\n</code></pre> <p>An example yaml configuration for this is as follows:</p> <p>FIXME https://github.com/Nota-NetsPresso/netspresso-trainer/issues/150</p> <pre><code>\n</code></pre>"},{"location":"components/data/#object-detection","title":"Object detection","text":"<p>To train an object detection model using NetsPresso Trainer, the data must be in following formats: </p> <ul> <li>For object model training, there must be a <code>.txt</code> file for each training image indicating the original image and the bounding box and class index corresponding to each bounding box of the image.</li> <li>The format of the bounding box follows the YOLO dataset format ([x_center, y_center, width, height], normalized).</li> <li>Each <code>.txt</code> file must contain one line for each bounding box.</li> <li>In this case, training data and validation data can be distinguished in different directories. For example, training data can be placed in train/image, train/label directories, and validation data can be placed in valid/image, valid/label directories.</li> <li>Users must know the class name corresponding to each class index in the label file.</li> </ul> <p>The example data directory structure for this is as follows: </p> <pre><code># TODO\n</code></pre> <p>An example yaml configuration for this is as follows: </p> <pre><code># This example dataset is downloaded from &lt;https://www.kaggle.com/code/valentynsichkar/traffic-signs-detection-by-yolo-v3-opencv-keras/input&gt;\ndata:\nname: traffic_sign_yolo\ntask: detection\nformat: local # local, huggingface\npath:\nroot: ../../data/traffic-sign # dataset root\ntrain:\nimage: images/train # directory for training images\nlabel: labels/train # directory for training labels\nvalid:\nimage: images/val  # directory for valid images\nlabel: labels/val  # directory for valid labels\ntest:\nimage: ~  # directory for test images\nlabel: ~  # directory for test labels\npattern:\nimage: ~\nlabel: ~\nid_mapping: ['prohibitory', 'danger', 'mandatory', 'other']  # class names\npallete: ~\n</code></pre>"},{"location":"components/data/#training-with-hugging-face-datasets","title":"Training with Hugging Face datasets","text":"<p>NetsPresso Trainer is striving to support various dataset hubs and platforms.  As part of that effort and first step, NetsPresso Trainer can be used with data in Hugging Face datasets. </p> <p>An example configuration for Hugging Face datasets is as follows: </p> <pre><code>data:\nname: beans\ntask: classification\nformat: huggingface\nmetadata:\ncustom_cache_dir: ./data/huggingface repo: beans\nsubset: ~\nfeatures:\nimage: image\nlabel: labels\n</code></pre>"},{"location":"components/data/#field-list","title":"Field list","text":""},{"location":"components/data/#local-dataset","title":"Local dataset","text":""},{"location":"components/data/#common","title":"Common","text":"Field  Description <code>data.name</code> (str) the name of dataset <code>data.task</code> (str) <code>classification</code> for image classification, <code>segmentation</code> for semantic segmentation, and <code>detection</code> for object detection <code>data.format</code> <code>local</code> as an identifier of dataset format <code>data.path.root</code> (str) root directory of dataset <code>data.path.train.image</code> (str) training image directory. Should be relative path to root directory. <code>data.path.train.label</code> (str) training label directory. Should be relative path to root directory. <code>data.path.valid.image</code> (str) validation image directory. Should be relative path to root directory. <code>data.path.valid.label</code> (str) validation label directory. Should be relative path to root directory. <code>data.path.test.image</code> (str) image directory for test dataset. Should be relative path to root directory. <code>data.path.test.label</code> (str) image directory for test dataset. Should be relative path to root directory."},{"location":"components/data/#classification","title":"Classification","text":"Field  Description <code>data.path.id_mapping</code> (dict) key-value pair between directory name and class name. Should be a list of (dirname: classname)."},{"location":"components/data/#segmentation","title":"Segmentation","text":"Field  Description <code>data.path.id_mapping</code> (dict) key-value pair between pixel value (RGB or image type) and class name. Should be a list of ((R, G, B): classname). <code>data.path.palette</code> (dict) Color mapping for visualization. If <code>none</code>, automatically select the color for each class."},{"location":"components/data/#detection","title":"Detection","text":"Field  Description <code>data.path.id_mapping</code> (list) class list for each class index <code>data.path.palette</code> (dict) Color mapping for visualization. If <code>none</code>, automatically select the color for each class."},{"location":"components/data/#hugging-face-datasets","title":"Hugging Face datasets","text":"Field  Description <code>data.name</code> (str) the name of dataset <code>data.task</code> (str) <code>classification</code> for image classification, <code>segmentation</code> for semantic segmentation, and <code>detection</code> for object detection <code>data.format</code> <code>huggingface</code> as an identifier of dataset format <code>data.metadata.custom_cache_dir</code> (str) cache directory to load and save dataset files from Hugging Face <code>data.metadata.repo</code> (str) Repository name. (e.g. <code>competitions/aiornot</code> represents the dataset <code>huggingface.co/datasets/competitions/aiornot</code>.) <code>data.metadata.subset</code> (str, optional) subset name if the dataset contains multiple versions <code>data.metadata.features.image</code> (str) key which represents the image at the dataset header <code>data.metadata.features.label</code> (str) key which represents the label at the dataset header"},{"location":"components/logging/","title":"Logging","text":""},{"location":"components/logging/#tensorboard","title":"Tensorboard","text":"<p>We provide basic tensorboard to track your training status. Run the tensorboard with the following command: </p> <pre><code>tensorboard --logdir ./outputs --port 50001 --bind_all\n</code></pre> <p>where <code>PORT</code> for tensorboard is 50001. Note that the default directory of saving result will be <code>./outputs</code> directory.</p>"},{"location":"components/training/","title":"Overview","text":"<p>In training, the training recipe is just as important as the model architecture. Even if you have a good model architecture, the performance on the same data and model combination can vary greatly depending on the training recipe. NetsPresso Trainer not only introduces models optimized for edge devices, but also provides the ability to change training configurations to train these models with various data. The optimal training recipe will vary depending on the data you want to train. Use the options provided by NetsPresso Trainer to find the best training recipe for your data.</p>"},{"location":"components/training/#batch-size-and-epochs","title":"Batch size and epochs","text":"<p>The batch size and epoch used in training may vary depending on the GPU and server specifications you have. If the batch size is large, it tends to take up a lot of GPU memory, and as the number of epochs trained increases, it tends to take a long time to complete the training. Adjust the values according to your server specifications, but for successful training, it is recommended to set the batch size to at least 8.</p>"},{"location":"components/training/#optimizers","title":"Optimizers","text":"<p>NetsPresso Trainer uses the optimizers implemented in PyTorch as is. By selecting an optimizer suitable for the training recipe, including batch size, you can configure the optimal training.</p>"},{"location":"components/training/#supporting-optimizers","title":"Supporting optimizers","text":"<ul> <li>AdamW (<code>adamw</code>)</li> <li>Adam (<code>adam</code>)</li> <li>Adadelta (<code>adadelta</code>)</li> <li>Adagrad (<code>adagrad</code>)</li> <li>RMSprop (<code>rmsprop</code>)</li> <li>Adamax (<code>adamax</code>)</li> <li>SGD (<code>sgd</code>)</li> </ul> <p>If you are unsure which optimizer to use, we recommend reading the blog post from towardsdatascience.</p>"},{"location":"components/training/#learning-rate-scheduler","title":"Learning rate scheduler","text":"<p>NetsPresso Trainer supports various learning rate schedulers based on PyTorch. In particular, warm-up is supported for frequently used learning rate schedulers, and warm restart is supported for some schedulers, such as cosine annealing. NetsPresso Trainer updates the learning rate at the end of epoch, not the end of step, so users will set the scheduler with epoch-level counts.</p>"},{"location":"components/training/#supporting-schedulers","title":"Supporting schedulers","text":""},{"location":"components/training/#gradio-demo-for-simulating-the-learning-rate-scheduler","title":"Gradio demo for simulating the learning rate scheduler","text":"<p>In many training feature repositories, it is recommended to perform the entire training pipeline and check the log to see how the learning rate scheduler works. NetsPresso Trainer supports learning rate schedule simulation to allow users to easily understand the learning rate scheduler for their configured training recipe. By copying the training configuration into the simulator, users can see how the learning rate changes every epoch.</p> <p> This simulation is not supported for some schedulers which adjust the learning rate dynamically with training results.</p>"},{"location":"components/training/#running-on-your-environment","title":"Running on your environment","text":"<p>FIXME</p>"},{"location":"components/training/#hugging-face-spaces","title":"Hugging Face Spaces","text":"<p>(HF Spaces demo link)</p>"},{"location":"components/training/#field-list","title":"Field list","text":"Field  Description <code>training.seed</code> (int) random seed <code>training.opt</code> (str) the type of optimizer. Please check the list of supporting optimizer for more details. <code>training.lr</code> (float) base learning rate <code>training.momentum</code> (float) momentum value for optimizer <code>training.weight_decay</code> (float) the strength of L2 penalty <code>training.sched</code> (str) the type of scheduler. Please check the list of supporting LR schedulers for more details. <code>training.min_lr</code> (float) the minimum value of learning rate <code>training.warmup_bias_lr</code> (float) the starting learning rate of warmup period in learning rate scheduling <code>training.warmup_epochs</code> (int) the warmup period <code>training.iters_per_phase</code> (int) the period of base phase in learning rate scheduling. Applied when the scheduler is <code>step</code> or <code>cosine</code>. <code>training.sched_power</code> (float) the power value of polynomial scheduler. Applied when the scheduler is <code>poly</code>. <code>training.epochs</code> (int) the total number of epoch for training the model <code>training.batch_size</code> (int) the number of samples in single batch input"},{"location":"components/model/field/","title":"Field list","text":""},{"location":"components/model/field/#common","title":"Common","text":""},{"location":"components/model/field/#architecture","title":"Architecture","text":""},{"location":"components/model/field/#backbone-resnet","title":"backbone - resnet","text":""},{"location":"components/model/field/#backbone-segformer","title":"backbone - segformer","text":""},{"location":"components/model/field/#backbone-vit","title":"backbone - vit","text":""},{"location":"components/model/field/#backbone-mobilevit","title":"backbone - mobilevit","text":""},{"location":"components/model/field/#backbone-efficientformer","title":"backbone - efficientformer","text":""},{"location":"components/model/field/#head-fc","title":"head - fc","text":""},{"location":"components/model/field/#head-all_mlp_head","title":"head - all_mlp_head","text":""},{"location":"components/model/field/#head-decode_head","title":"head - decode_head","text":""},{"location":"components/model/field/#full-pidnet","title":"full - pidnet","text":""},{"location":"components/model/field/#loss","title":"Loss","text":""},{"location":"models/","title":"Index","text":""},{"location":"models/#models","title":"Models","text":""},{"location":"models/#features","title":"Features","text":"<ul> <li>Compatible with torch.fx converting</li> <li>Can be compressed with pruning method provided in NetsPresso</li> <li>Efficient to be easily deployed at many edge devices.</li> </ul>"},{"location":"models/#vit-and-metaformer","title":"ViT and MetaFormer","text":"<p>MetaFormer, introduced by Sea AI Lab, suggested the abstracted architecture of Vision Transformer and has widely applied in recent vision backbone architectures with better performance. The framework of MetaFormer not just covers the original ViT based models, but also explains some hybrid models of ConvNets and ViT in single framework. Especially, those models set new record in Efficient vision task, which show better performance in both accuracy and inference speed.  </p> <p>From the MetaFormer paper, the concept of MetaFormer can be expressed as follows:</p> \\[ X = \\mathrm{InputEmbedding}(I), \\newline X' = \\mathrm{MetaFormerEncoder}(X), \\newline X'' = \\mathrm{NormOrIdentity_{output}}(X'), \\] <p>where \\(\\mathrm{MetaFormerEncoder}\\) usually consists of repeated MetaFormer blocks.</p> <p>One of MetaFormer blocks can be expressed as follows:</p> <p>$$ X' = X + \\mathrm{TokenMixer}\\left(\\mathrm{Norm_1}(X)\\right), \\newline X'' = X' + \\mathrm{ChannelMLP}\\left(\\mathrm{Norm_2}(X')\\right), $$ where \\(\\mathrm{TokenMixer}\\) could be either \\(\\mathrm{MultiHeadSelfAttention}\\), \\(\\mathrm{Identity}\\), or \\(\\mathrm{ConvLayers}\\), and \\(\\mathrm{ChannelMLP}\\) usually consists of <code>Linear</code> layers and activation function.</p> <p>Inspired from the MetaFormer research, we try out best to design and build models (both ViT and non-ViT models) with MetaFormer framework to utilize in various ways. By defining models with unified MetaFormer block, we can apply to all models whenever NetsPresso services progressed. For example, Compressor API of PyNetsPresso supports structural pruning for those models, which accelerates the inference only with negotiable performance drop.</p>"},{"location":"models/#retraining-the-model-from-netspresso","title":"Retraining the model from NetsPresso","text":"<p>If you got compressed model from NetsPresso, then it's time to retrain your model to get the best performance.</p>"},{"location":"models/implementations/","title":"Implementations","text":""},{"location":"models/model-list/","title":"Index","text":""},{"location":"models/model-list/#pretrained-weights","title":"Pretrained weights","text":"<p>For now, we provide the pretrained weight from other awesome repositories. We have converted several models' weights into our own model architectures. In the near soon, we are planning to provide the pretrained weights directly trained from our resources. We appreciate all the original authors and we also do our best to make other values.</p> <p>Download all weights (Google Drvie)</p> Family Model Link Origianl repository ResNet <code>resnet50</code> Google Drive torchvision ViT <code>vit_tiny</code> Google Drive apple/ml-cvnets MobileViT <code>mobilevit_s</code> Google Drive apple/ml-cvnets SegFormer <code>segformer</code> Google Drive (Hugging Face) nvidia EfficientForemer <code>efficientformer_l1_3000d</code> Google Drive snap-research/EfficientFormer PIDNet <code>pidnet_s</code> Google Drive XuJiacong/PIDNet"},{"location":"models/model-list/backbones/","title":"Models","text":"<p> Note that all FLOPs and # Params values in each task section includes backbones, not only head itself.</p>"},{"location":"models/model-list/backbones/#resnet","title":"ResNet","text":"<ul> <li>Original Paper: Deep Residual Learning for Image Recognition</li> <li>Related Links<ul> <li><code>pytorch/vision</code></li> </ul> </li> </ul> Model Supporting Task(s) torch.fx NetsPresso Checkpoint <code>resnet50</code> Classification \u2705 \u2705 Google Drive"},{"location":"models/model-list/backbones/#classification","title":"Classification","text":"Head FLOPs # Params Speed Benchmark result(s) <code>fc</code> - - - -"},{"location":"models/model-list/backbones/#vit","title":"ViT","text":"<ul> <li>Original Paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</li> <li>Related Links<ul> <li><code>apple/ml-cvnets</code></li> </ul> </li> </ul> Model Supporting Task(s) torch.fx NetsPresso Checkpoint <code>vit_tiny</code> Classification \u2705 \u2705 Google Drive"},{"location":"models/model-list/backbones/#classification_1","title":"Classification","text":"Head FLOPs # Params Speed Benchmark result(s) <code>fc</code> - - - -"},{"location":"models/model-list/backbones/#mobilevit","title":"MobileViT","text":"<ul> <li>Original Paper: MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</li> <li>Related Links<ul> <li><code>apple/ml-cvnets</code></li> </ul> </li> </ul> Model Supporting Task(s) torch.fx NetsPresso Checkpoint <code>mobilevit_s</code> Classification \u2705 \u2705 Google Drive"},{"location":"models/model-list/backbones/#classification_2","title":"Classification","text":"Head FLOPs # Params Speed Benchmark result(s) <code>fc</code> - - - -"},{"location":"models/model-list/backbones/#segformer","title":"SegFormer","text":"<ul> <li>Original Paper: SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</li> <li>Related Links<ul> <li><code>huggingface/transformers</code></li> </ul> </li> </ul> Model Supporting Task(s) torch.fx NetsPresso Checkpoint <code>segformer</code> Classification,Segmentation \u2705 \u2705 Google Drive"},{"location":"models/model-list/backbones/#classification_3","title":"Classification","text":"Head FLOPs # Params Speed Benchmark result(s) <code>fc</code> - - - -"},{"location":"models/model-list/backbones/#segmentation","title":"Segmentation","text":"Head FLOPs # Params Speed Benchmark result(s) <code>decode_head</code> - - - -"},{"location":"models/model-list/backbones/#efficientformer","title":"EfficientFormer","text":"<ul> <li>Original Paper: EfficientFormer: Vision Transformers at MobileNet Speed</li> <li>Related Links<ul> <li><code>snap-research/EfficientFormer</code></li> </ul> </li> </ul> Model Supporting Task(s) torch.fx NetsPresso Checkpoint <code>segformer</code> Classification,Segmentation \u2705 \u2705 Google Drive"},{"location":"models/model-list/backbones/#classification_4","title":"Classification","text":"Head FLOPs # Params Speed Benchmark result(s) <code>fc</code> - - - -"},{"location":"models/model-list/backbones/#segmentation_1","title":"Segmentation","text":"Head FLOPs # Params Speed Benchmark result(s) <code>decode_head</code> - - - -"},{"location":"models/model-list/full-models/","title":"Models","text":""},{"location":"models/model-list/full-models/#pidnet","title":"PIDNet","text":"<ul> <li>Original Paper: PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers</li> <li>Related Links<ul> <li><code>XuJiacong/PIDNet</code></li> </ul> </li> </ul> Model Supporting Task(s) torch.fx NetsPresso Checkpoint <code>pidnet_s</code> Segmentation \u2705 \u2705 Google Drive FLOPs # Params Speed Benchmark result(s) - - - -"},{"location":"models/model-list/heads/","title":"Heads","text":""},{"location":"models/model-list/heads/#head-architecutres","title":"Head Architecutres","text":""},{"location":"models/model-list/heads/#classification","title":"Classification","text":"Architecture torch.fx Supporting Models <code>fc</code> \u2705 (All backbones)"},{"location":"models/model-list/heads/#segmentation","title":"Segmentation","text":"Architecture torch.fx Supporting Models <code>segment_head</code> \u2705 SegFormerEfficientFormer"},{"location":"models/model-list/heads/#detection","title":"Detection","text":"<p> <code>2023.08.08</code> Work in progress for detection</p>"},{"location":"pynetspresso/","title":"Index","text":"<p>Use PyNetsPresso for a seamless model optimization process.  </p> <p>PyNetsPresso resolves AI-related constraints in business use cases and enables cost-efficiency and enhanced performance by removing the requirement for high-spec servers and network connectivity and preventing high latency and personal data breaches.</p> <p>PyNetsPresso is a python interface with the NetsPresso web application and REST API.</p> <p>Easily compress various models with our resources. Please browse the Docs for details, and join our Discussion Forum for providing feedback or sharing your use cases.</p> <p>To get started with the PyNetsPresso, you will need to sign up either at NetsPresso or PyNetsPresso.</p> <p> </p>"},{"location":"pynetspresso/#installation","title":"Installation","text":"<p>There are two ways you can install the PyNetsPresso: 1) using pip or 2) manually through our project GitHub repository.</p> <p>To install this package, please use Python 3.8 or higher.</p> <p>From PyPI (Recommended) <pre><code>pip install netspresso\n</code></pre></p> <p>From GitHub <pre><code>git clone https://github.com/nota-netspresso/pynetspresso.git\ncd pynetspresso\npip install -e .\n</code></pre></p>"},{"location":"pynetspresso/#quick-start","title":"Quick Start","text":""},{"location":"pynetspresso/#login","title":"Login","text":"<p>To use the PyNetsPresso, please enter the email and password registered in NetsPresso.</p> <pre><code>from netspresso.client import SessionClient\nfrom netspresso.compressor import ModelCompressor\nsession = SessionClient(email='YOUR_EMAIL', password='YOUR_PASSWORD')\ncompressor = ModelCompressor(user_session=session)\n</code></pre>"},{"location":"pynetspresso/#upload-model","title":"Upload Model","text":"<p>To upload your trained model, simply enter the required information. </p> <p>When a model is successfully uploaded, a unique <code>model.model_id</code> is generated to allow repeated use of the uploaded model.</p> <pre><code>from netspresso.compressor import Task, Framework\nmodel = compressor.upload_model(\nmodel_name=\"YOUR_MODEL_NAME\",\ntask=Task.IMAGE_CLASSIFICATION,\nframework=Framework.TENSORFLOW_KERAS,\nfile_path=\"YOUR_MODEL_PATH\", # ex) ./model.h5\ninput_shapes=\"YOUR_MODEL_INPUT_SHAPES\",  # ex) [{\"batch\": 1, \"channel\": 3, \"dimension\": [32, 32]}]\n)\n</code></pre> <p> </p>"},{"location":"pynetspresso/compressor/","title":"Compressor","text":""},{"location":"pynetspresso/compressor/#automatic-compression","title":"Automatic Compression","text":"<p>Automatically compress the model by setting the compression ratio for the model.</p> <p>Enter the ID of the uploaded model, the name and storage path of the compressed model, and the compression ratio.</p> <pre><code>compressed_model = compressor.automatic_compression(\nmodel_id=model.model_id,\nmodel_name=\"YOUR_COMPRESSED_MODEL_NAME\",\noutput_path=\"OUTPUT_PATH\",  # ex) ./compressed_model.h5\ncompression_ratio=0.5,\n)\n</code></pre>"},{"location":"pynetspresso/launcher/","title":"Launcher","text":""},{"location":"pynetspresso/launcher/#convert-model-and-benchmark-the-converted-model","title":"Convert Model and Benchmark the Converted Model","text":"<p>Convert an ONNX model into a TensorRT model, and benchmark the TensorRT model on the Jetson Nano.</p> <pre><code>from loguru import logger\nfrom netspresso.launcher import ModelConverter, ModelBenchmarker, ModelFramework, TaskStatus, DeviceName, SoftwareVersion\nconverter = ModelConverter(user_session=session)\nmodel = converter.upload_model(\"./examples/sample_models/test.onnx\")\nconversion_task = converter.convert_model(\nmodel=model,\ninput_shape=model.input_shape,\ntarget_framework=ModelFramework.TENSORRT,\ntarget_device_name=DeviceName.JETSON_AGX_ORIN,\ntarget_software_version=SoftwareVersion.JETPACK_5_0_1,\nwait_until_done=True\n)\nlogger.info(conversion_task)\nCONVERTED_MODEL_PATH = \"converted_model.trt\"\nconverter.download_converted_model(conversion_task, dst=CONVERTED_MODEL_PATH)\nbenchmarker = ModelBenchmarker(user_session=session)\nbenchmark_model = benchmarker.upload_model(CONVERTED_MODEL_PATH)\nbenchmark_task = benchmarker.benchmark_model(\nmodel=benchmark_model,\ntarget_device_name=DeviceName.JETSON_AGX_ORIN,\ntarget_software_version=SoftwareVersion.JETPACK_5_0_1,\nwait_until_done=True\n)\nlogger.info(f\"model inference latency: {benchmark_task.latency} ms\")\nlogger.info(f\"model gpu memory footprint: {benchmark_task.memory_footprint_gpu} MB\")\nlogger.info(f\"model cpu memory footprint: {benchmark_task.memory_footprint_cpu} MB\")\n</code></pre>"},{"location":"pynetspresso/launcher/#available-devices-with-pynetspresso-launcher-convert-benchmark","title":"Available Devices with PyNetsPresso Launcher (Convert, Benchmark)","text":"<p>To fully use PyNetsPresso Launcher, model checkpoints from PyTorch has to be provided with onnx format. From our trainer, you can export the model checkpoint with onnx format when training is finished. With onnx files, the following devices are executable with PyNetsPresso Launcher:</p> <ul> <li>Raspberry Pi<ul> <li>4 Mobel B</li> <li>3 Model B+</li> <li>Zero W</li> <li>Zero</li> </ul> </li> <li>Renesas Embedded AI MPUs<ul> <li>RZ/V2L</li> <li>RZ/V2M</li> </ul> </li> <li>NVIDIA Jetson<ul> <li>Nano</li> <li>TX2</li> <li>Xavier</li> <li>Nx</li> <li>AGX Orin</li> </ul> </li> <li>AWS instance<ul> <li>T4  </li> </ul> </li> </ul> <p>For more details, please refer to compatibility matrix in PyNetsPresso.</p> <p> </p>"},{"location":"pynetspresso/torch-fx/","title":"torch.fx","text":""},{"location":"pynetspresso/torch-fx/#pytorch-graphmodule-with-fx-tracer","title":"PyTorch GraphModule with fx tracer","text":"<p>To fully use with PyNetsPresso, the model checkpoint from trainer should be converted to symbolic traced format. Thanks to PyTorch team, torch FX is provided as a toolkit for developers to transform <code>nn.Module</code> with symbolic tracing, graph representation, and python code generation. Please refer to torch FX docuement for more details.</p> <p>Our trainer provides the trained model checkpoint with both onnx format (<code>.onnx</code>) and graphmodule format (<code>.pt</code>). So, you don't have to convert the model manually after training. The followings are several code blocks about loading and converting FX graph.</p>"},{"location":"pynetspresso/torch-fx/#graphmodule-convert","title":"GraphModule convert","text":"<pre><code>import torch.fx\nfrom torchvision.models import resnet18, ResNet18_Weights\nmodel = resnet18(weights=ResNet18_Weights)\ngraph = torch.fx.Tracer().trace(model)\ntraced_model = torch.fx.GraphModule(model, graph)\ntorch.save(traced_model, \"resnet18.pt\")\n</code></pre>"},{"location":"pynetspresso/torch-fx/#grahmodule-inference","title":"GrahModule inference","text":"<pre><code>import torch\nimport torch.fx\nimport numpy as np\nfrom torchvision.models import resnet18, ResNet18_Weights\nmodel = resnet18(weights=ResNet18_Weights)\ngraph = torch.fx.Tracer().trace(model)\ntraced_model = torch.fx.GraphModule(model, graph)\n# input size is needed to be choosen\ninput_shape = (1, 3, 224, 224)\nrandom_input = torch.Tensor(np.random.randn(*input_shape))\nwith torch.no_grad():\noriginal_output = model(random_input)\ntraced_output = traced_model(random_input)\nassert torch.allclose(original_output, traced_output), \"inference result is not equal!\"\n</code></pre>"},{"location":"pynetspresso/torch-fx/#load-graphmodule","title":"Load GraphModule","text":"<pre><code>import torch\nmodel = torch.load(\"resnet18.pt\")\n</code></pre>"},{"location":"pynetspresso/best-practice/benchmark-with-launcher/","title":"Benchmark with Jetson AGX Orin","text":""},{"location":"pynetspresso/best-practice/benchmark-with-launcher/#resnet-classification-model-with-pynetspresso-launcher","title":"ResNet classification model with PyNetsPresso Launcher","text":""},{"location":"pynetspresso/best-practice/benchmark-with-launcher/#train-your-resnet50-model","title":"Train your ResNet50 model","text":"<p>Train your model with your own dataset. </p>"},{"location":"pynetspresso/best-practice/benchmark-with-launcher/#with-pynetspresso-launcher","title":"With PyNetsPresso Launcher","text":"<p>Please write the path of your onnx checkpoint from training result, and replace the example checkpoint path with your file:</p> <pre><code>model = converter.upload_model(\"YOUR/CHECKPOINT/PATH\")\n</code></pre> <p>Please refer to PyNetsPresso Launcher Guide for more details.</p>"},{"location":"pynetspresso/best-practice/benchmark-with-launcher/#benchmark-result","title":"Benchmark Result","text":"<p>The following is an example result tested with Jetson AGX Orin with TensorRT backend:</p> <pre><code>root@ee7398d3c015:/home/appuser/netspresso-trainer/np-compatibility# python tools/pynetspresso.py \n2023-08-28 03:34:44.206 | INFO     | netspresso.client.config:&lt;module&gt;:10 - Read prod config\n2023-08-28 03:34:44.566 | INFO     | netspresso.client:__login:50 - Login successfully\n2023-08-28 03:34:45.082 | INFO     | netspresso.client:__get_user_info:67 - successfully got user information\n2023-08-28 03:35:05.647 | INFO     | netspresso.launcher:convert_model:104 - Converting Model for Jetson-AGX-Orin (tensorrt)\n2023-08-28 03:36:24.818 | INFO     | __main__:&lt;module&gt;:20 - user_uuid='d3aa4f44-2237-4a4a-8c74-ee58e43d9472' input_model_uuid='d8727c33-2b56-4b76-ac79-2f71c579951c' status='FINISHED' input_shape=InputShape(batch=1, channel=3, input_size='256, 256') data_type='FP16' software_version='5.0.1-b118' framework='onnx' convert_task_uuid='ef8eea24-9def-4bfe-80d0-9b48986f62f6' output_model_uuid='15d4d3d1-6ac1-4e3e-acc9-cfe997184337' model_file_name='model.trt' target_device_name='Jetson-AGX-Orin'\n2023-08-28 03:36:36.240 | INFO     | netspresso.launcher:download_converted_model:171 - The file has been successfully downloaded at : converted_model.trt\n2023-08-28 03:39:54.680 | INFO     | __main__:&lt;module&gt;:34 - model inference latency: 1.2281 ms\n2023-08-28 03:39:54.682 | INFO     | __main__:&lt;module&gt;:35 - model gpu memory footprint: 49.0 MB\n2023-08-28 03:39:54.683 | INFO     | __main__:&lt;module&gt;:36 - model cpu memory footprint: 302.0 MB\n</code></pre> <p> </p>"},{"location":"pynetspresso/best-practice/compress-and-retrain/","title":"Compress and Retrain","text":""},{"location":"pynetspresso/best-practice/compress-and-retrain/#segformer-model-with-pynetspresso-compressor","title":"SegFormer model with PyNetsPresso Compressor","text":""},{"location":"pynetspresso/best-practice/compress-and-retrain/#structural-pruning-with-automatic-compression","title":"Structural pruning with automatic compression","text":"<p>Succeess in training compressed model.</p>"},{"location":"pynetspresso/best-practice/compress-and-retrain/#retrain-with-your-compressed-model","title":"Retrain with your compressed model","text":"<p>After compressing the model with PyNetsPresso Compressor, we recommend you to re-train the compressed model with your own dataset. We'll provide an example script for re-training the compressed model with our trainer.</p> <pre><code>python train_fx.py\\\n--data config/data/beans.yaml\\\n--augmentation config/augmentation/segmentation.yaml\\\n--model config/model/segformer/segformer.yaml\\\n--training config/training/segmentation.yaml\\\n--logging config/logging.yaml\\\n--environment config/environment.yaml\\\n--fx-model-checkpoint segmentation_segformer_fx.pt\n</code></pre> <p>Here is the example tensorboard image in the below, where orange line is the training result compressed with <code>GlobalPruning</code> with the ratio of <code>0.4</code>, and the  skyblue line is the original training result (finetuning from ImageNet pretrained weight).</p> <p></p> <p> </p>"}]}